<p align="center">
<img alt="Agentic RAG for Dummies Logo" src="assets/logo.png" width="350px">
</p>

<h1 align="center">Agentic RAG å…¥é—¨æŒ‡å—</h1>

<p align="center">
  <strong>ä½¿ç”¨ LangGraphã€å¯¹è¯è®°å¿†å’Œäººåœ¨ç¯ä¸­çš„æŸ¥è¯¢æ¾„æ¸…åŠŸèƒ½æ„å»ºç”Ÿäº§çº§ Agentic RAG ç³»ç»Ÿ</strong>
</p>

<p align="center">
  <a href="#æ¦‚è¿°">æ¦‚è¿°</a> â€¢
  <a href="#å·¥ä½œåŸç†">å·¥ä½œåŸç†</a> â€¢
  <a href="#llm-æä¾›å•†é…ç½®">LLM æä¾›å•†</a> â€¢
  <a href="#å®ç°">å®ç°</a> â€¢
  <a href="#å®‰è£…ä¸ä½¿ç”¨">å®‰è£…ä¸ä½¿ç”¨</a> â€¢
  <a href="#æ•…éšœæ’é™¤">æ•…éšœæ’é™¤</a>
</p>

<p align="center">
  <strong>å¿«é€Ÿå¼€å§‹ ğŸ‘‰</strong> 
  <a href="https://colab.research.google.com/gist/GiovanniPasq/ddfc4a09d16b5b97c5c532b5c49f7789/agentic_rag_for_dummies.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="åœ¨ Colab ä¸­æ‰“å¼€"/>
  </a>
</p>

<p align="center">
  <img alt="Agentic RAG Demo" src="assets/demo.gif" width="650px">
</p>

<p align="center">
  <strong>å¦‚æœå–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Œç»™ä¸ª star â­ï¸ å§ :)</strong>
</p>

## æ¦‚è¿°

æœ¬ä»“åº“æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LangGraph ä»¥æœ€å°‘çš„ä»£ç æ„å»ºä¸€ä¸ª **Agentic RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰** ç³»ç»Ÿã€‚å®ƒå®ç°äº†ï¼š

- ğŸ’¬ **å¯¹è¯è®°å¿†**ï¼šåœ¨å¤šä¸ªé—®é¢˜ä¸­ä¿æŒä¸Šä¸‹æ–‡ï¼Œå®ç°è‡ªç„¶å¯¹è¯
- ğŸ”„ **æŸ¥è¯¢æ¾„æ¸…**ï¼šè‡ªåŠ¨é‡å†™æ¨¡ç³Šçš„æŸ¥è¯¢æˆ–è¯·æ±‚æ¾„æ¸…
- ğŸ” **åˆ†å±‚ç´¢å¼•**ï¼šæœç´¢å°è€Œå…·ä½“çš„å—ï¼ˆå­å—ï¼‰ä»¥è·å¾—ç²¾ç¡®æ€§ï¼Œæ£€ç´¢æ›´å¤§çš„çˆ¶å—ä»¥è·å–ä¸Šä¸‹æ–‡
- ğŸ¤– **æ™ºèƒ½ä½“ç¼–æ’**ï¼šä½¿ç”¨ LangGraph åè°ƒæ•´ä¸ªå·¥ä½œæµç¨‹
- ğŸ§  **æ™ºèƒ½è¯„ä¼°**ï¼šåœ¨ç»†ç²’åº¦å—çº§åˆ«è¯„ä¼°ç›¸å…³æ€§
- âœ… **è‡ªæˆ‘çº æ­£**ï¼šå¦‚æœåˆå§‹ç»“æœä¸è¶³ï¼Œé‡æ–°æŸ¥è¯¢
- ğŸ”€ **å¤šæ™ºèƒ½ä½“ Map-Reduce**ï¼šå°†æŸ¥è¯¢åˆ†è§£ä¸ºå¹¶è¡Œå­æŸ¥è¯¢ï¼Œä»¥è·å¾—å…¨é¢çš„ç­”æ¡ˆ

---

### ğŸ¯ ä½¿ç”¨æœ¬ä»“åº“çš„ä¸¤ç§æ–¹å¼

**1ï¸âƒ£ å­¦ä¹ è·¯å¾„ï¼šäº¤äº’å¼ç¬”è®°æœ¬**  
é€æ­¥æ•™ç¨‹ï¼Œéå¸¸é€‚åˆç†è§£æ ¸å¿ƒæ¦‚å¿µã€‚å¦‚æœæ‚¨æ˜¯ Agentic RAG çš„æ–°æ‰‹æˆ–æƒ³å¿«é€Ÿå®éªŒï¼Œä»è¿™é‡Œå¼€å§‹ã€‚ä¸“æ³¨äºåŸºæœ¬å·¥ä½œæµç¨‹ï¼Œä¸ä½¿ç”¨é«˜çº§åŠŸèƒ½ï¼Œä»¥ä¿æŒç®€å•ã€‚

**2ï¸âƒ£ æ„å»ºè·¯å¾„ï¼šæ¨¡å—åŒ–é¡¹ç›®**  
æ¨¡å—åŒ–æ¶æ„ï¼Œæ¯ä¸ªç»„ä»¶éƒ½å¯ä»¥ç‹¬ç«‹æ›¿æ¢ã€‚å¦‚æœæ‚¨æƒ³æ„å»ºå®é™…åº”ç”¨æˆ–æ ¹æ®éœ€è¦è‡ªå®šä¹‰ç³»ç»Ÿï¼Œè¯·ä½¿ç”¨æ­¤æ–¹æ³•ã€‚

**æ‚¨å¯ä»¥è‡ªå®šä¹‰çš„å†…å®¹ç¤ºä¾‹ï¼š**
- **LLM æä¾›å•†**ï¼šä» Ollama ä¸€è¡Œä»£ç åˆ‡æ¢åˆ° Claudeã€OpenAI æˆ– Gemini
- **æ™ºèƒ½ä½“å·¥ä½œæµç¨‹**ï¼šåœ¨å›¾ä¸­æ·»åŠ /åˆ é™¤èŠ‚ç‚¹ï¼Œå¹¶ä¸ºç‰¹å®šé¢†åŸŸï¼ˆæ³•å¾‹ã€åŒ»ç–—ç­‰ï¼‰è‡ªå®šä¹‰ç³»ç»Ÿæç¤º
- **PDF è½¬æ¢**ï¼šç”¨ Doclingã€PaddleOCR æˆ–å…¶ä»–å·¥å…·æ›¿æ¢ PyMuPDF
- **åµŒå…¥æ¨¡å‹**ï¼šé€šè¿‡é…ç½®æ›´æ”¹å¯†é›†/ç¨€ç–åµŒå…¥æ¨¡å‹

è¯·å‚é˜…[æ¨¡å—åŒ–æ¶æ„](#æ¨¡å—åŒ–æ¶æ„)éƒ¨åˆ†äº†è§£ç³»ç»Ÿçš„ç»„ç»‡æ–¹å¼ï¼Œä»¥åŠ[å®‰è£…ä¸ä½¿ç”¨](#å®‰è£…ä¸ä½¿ç”¨)éƒ¨åˆ†å¼€å§‹ä½¿ç”¨ã€‚

---

è¿™ç§æ–¹æ³•ç»“åˆäº†**å°å—çš„ç²¾ç¡®æ€§**å’Œ**å¤§å—çš„ä¸Šä¸‹æ–‡ä¸°å¯Œæ€§**ï¼ŒåŒæ—¶ç†è§£å¯¹è¯æµç¨‹ã€è§£å†³æ¨¡ç³ŠæŸ¥è¯¢ï¼Œå¹¶é€šè¿‡å¹¶è¡Œæ™ºèƒ½ä½“å¤„ç†æ¥å¤„ç†å¤šé¢é—®é¢˜ã€‚**æ¨¡å—åŒ–æ¶æ„**ç¡®ä¿æ¯ä¸ªç»„ä»¶â€”â€”ä»æ–‡æ¡£å¤„ç†åˆ°æ£€ç´¢é€»è¾‘â€”â€”éƒ½å¯ä»¥è‡ªå®šä¹‰è€Œä¸ä¼šç ´åç³»ç»Ÿã€‚

---

## ä¸ºä»€ä¹ˆé€‰æ‹©æœ¬ä»“åº“ï¼Ÿ

å¤§å¤šæ•° RAG æ•™ç¨‹å±•ç¤ºåŸºæœ¬æ¦‚å¿µï¼Œä½†ç¼ºä¹ç”Ÿäº§å°±ç»ªæ€§ã€‚æœ¬ä»“åº“é€šè¿‡æä¾›**å­¦ä¹ ææ–™å’Œå¯éƒ¨ç½²ä»£ç **æ¥å¼¥åˆè¿™ä¸€å·®è·ï¼š

âŒ **å…¸å‹çš„ RAG ä»“åº“ï¼š**
- ç®€å•ç®¡é“åœ¨ç²¾ç¡®æ€§å’Œä¸Šä¸‹æ–‡ä¹‹é—´æƒè¡¡
- æ²¡æœ‰å¯¹è¯è®°å¿†
- é™æ€ã€éè‡ªé€‚åº”æ£€ç´¢
- éš¾ä»¥æ ¹æ®æ‚¨çš„ç”¨ä¾‹è‡ªå®šä¹‰
- æ²¡æœ‰ UI ç•Œé¢
- å•çº¿ç¨‹æŸ¥è¯¢å¤„ç†

âœ… **æœ¬ä»“åº“ï¼š**
- **ä¸¤æ¡å­¦ä¹ è·¯å¾„**ï¼šäº¤äº’å¼ç¬”è®°æœ¬ AND æ¨¡å—åŒ–é¡¹ç›®
- **åˆ†å±‚ç´¢å¼•**å®ç°ç²¾ç¡®æ€§ + ä¸Šä¸‹æ–‡
- **å¯¹è¯è®°å¿†**å®ç°è‡ªç„¶å¯¹è¯
- **äººåœ¨ç¯ä¸­**æŸ¥è¯¢æ¾„æ¸…
- **å¤šæ™ºèƒ½ä½“ Map-Reduce**å¹¶è¡Œå¤„ç†å¤æ‚æŸ¥è¯¢
- **æ¨¡å—åŒ–æ¶æ„** - å¯äº¤æ¢ä»»ä½•ç»„ä»¶
- **æä¾›å•†æ— å…³** - ä½¿ç”¨ä»»ä½• LLMï¼ˆOllamaã€OpenAIã€Claudeã€Geminiï¼‰
- **UI ç•Œé¢** - ç«¯åˆ°ç«¯ Gradio åº”ç”¨ï¼Œæ”¯æŒæ–‡æ¡£ç®¡ç†

---

## å·¥ä½œåŸç†

### æ–‡æ¡£å‡†å¤‡ï¼šåˆ†å±‚ç´¢å¼•

åœ¨å¤„ç†æŸ¥è¯¢ä¹‹å‰ï¼Œæ–‡æ¡£ä¼šè¢«åˆ†å‰²ä¸¤æ¬¡ä»¥å®ç°æœ€ä½³æ£€ç´¢ï¼š

- **çˆ¶å—**ï¼šåŸºäº Markdown æ ‡é¢˜ï¼ˆH1ã€H2ã€H3ï¼‰çš„å¤§åŒºå—
- **å­å—**ï¼šä»çˆ¶å—æ´¾ç”Ÿçš„å°å‹å›ºå®šå¤§å°å—

è¿™ç§æ–¹æ³•ç»“åˆäº†**å°å—çš„ç²¾ç¡®æ€§**ï¼ˆç”¨äºæœç´¢ï¼‰å’Œ**å¤§å—çš„ä¸Šä¸‹æ–‡ä¸°å¯Œæ€§**ï¼ˆç”¨äºç­”æ¡ˆç”Ÿæˆï¼‰ã€‚

---

### æŸ¥è¯¢å¤„ç†ï¼šå››é˜¶æ®µæ™ºèƒ½å·¥ä½œæµç¨‹
```
ç”¨æˆ·æŸ¥è¯¢ â†’ å¯¹è¯åˆ†æ â†’ æŸ¥è¯¢æ¾„æ¸… â†’
æ™ºèƒ½ä½“æ¨ç† â†’ æœç´¢å­å— â†’ è¯„ä¼°ç›¸å…³æ€§ â†’
ï¼ˆå¦‚éœ€è¦ï¼‰â†’ æ£€ç´¢çˆ¶å— â†’ ç”Ÿæˆç­”æ¡ˆ â†’ è¿”å›å“åº”
```

#### é˜¶æ®µ 1ï¼šå¯¹è¯ç†è§£
- åˆ†ææœ€è¿‘çš„å¯¹è¯å†å²ä»¥æå–ä¸Šä¸‹æ–‡
- åœ¨å¤šä¸ªé—®é¢˜ä¸­ä¿æŒå¯¹è¯è¿ç»­æ€§

#### é˜¶æ®µ 2ï¼šæŸ¥è¯¢æ¾„æ¸…

ç³»ç»Ÿæ™ºèƒ½å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼š
1. **è§£æå¼•ç”¨** - å°†"å¦‚ä½•æ›´æ–°å®ƒï¼Ÿ"è½¬æ¢ä¸º"å¦‚ä½•æ›´æ–° SQLï¼Ÿ"
2. **åˆ†è§£å¤æ‚é—®é¢˜** - å°†å¤šéƒ¨åˆ†é—®é¢˜åˆ†è§£ä¸ºèšç„¦çš„å­æŸ¥è¯¢
3. **æ£€æµ‹æ¨¡ç³ŠæŸ¥è¯¢** - è¯†åˆ«æ— æ„ä¹‰ã€ä¾®è¾±æ€§æˆ–æ¨¡ç³Šçš„é—®é¢˜
4. **è¯·æ±‚æ¾„æ¸…** - ä½¿ç”¨äººåœ¨ç¯ä¸­æš‚åœå¹¶è¯·æ±‚è¯¦ç»†ä¿¡æ¯
5. **é‡å†™ä¸ºæ£€ç´¢** - ä½¿ç”¨ç‰¹å®šã€å…³é”®è¯ä¸°å¯Œçš„è¯­è¨€ä¼˜åŒ–æŸ¥è¯¢

#### é˜¶æ®µ 3ï¼šæ™ºèƒ½æ£€ç´¢

**å¤šæ™ºèƒ½ä½“ Map-Reduce æ¶æ„ï¼š**

å½“æŸ¥è¯¢åˆ†æé˜¶æ®µè¯†åˆ«å‡ºå¤šä¸ªä¸åŒçš„é—®é¢˜ï¼ˆæ˜¾å¼è¯¢é—®æˆ–ä»å¤æ‚æŸ¥è¯¢åˆ†è§£ï¼‰æ—¶ï¼Œç³»ç»Ÿä½¿ç”¨ LangGraph çš„ `Send` API è‡ªåŠ¨ç”Ÿæˆå¹¶è¡Œæ™ºèƒ½ä½“å­å›¾ã€‚æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹é€šè¿‡å®Œæ•´æ£€ç´¢å·¥ä½œæµç¨‹å¤„ç†ä¸€ä¸ªé—®é¢˜ï¼š

1. æ™ºèƒ½ä½“æœç´¢å­å—ä»¥è·å¾—ç²¾ç¡®æ€§
2. è¯„ä¼°ç»“æœæ˜¯å¦è¶³å¤Ÿ
3. å¦‚éœ€è¦ï¼Œè·å–çˆ¶å—ä»¥è·å–ä¸Šä¸‹æ–‡
4. ä»å¯¹è¯ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
5. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè‡ªæˆ‘çº æ­£å¹¶é‡æ–°æŸ¥è¯¢

ç„¶åæ‰€æœ‰æ™ºèƒ½ä½“å“åº”è¢«èšåˆä¸ºç»Ÿä¸€çš„ç­”æ¡ˆã€‚

**ç¤ºä¾‹ï¼š** *"ä»€ä¹ˆæ˜¯ JavaScriptï¼Ÿä»€ä¹ˆæ˜¯ Pythonï¼Ÿ"* â†’ 2 ä¸ªå¹¶è¡Œæ™ºèƒ½ä½“åŒæ—¶æ‰§è¡Œ

**å•ä¸€é—®é¢˜å·¥ä½œæµç¨‹ï¼š**
å¯¹äºç®€å•æŸ¥è¯¢ï¼Œå•ä¸ªæ™ºèƒ½ä½“æ‰§è¡Œæ£€ç´¢å·¥ä½œæµç¨‹ï¼Œæ— éœ€å¹¶è¡ŒåŒ–ã€‚

#### é˜¶æ®µ 4ï¼šå“åº”ç”Ÿæˆ

ç³»ç»Ÿå°†æ£€ç´¢åˆ°çš„å—ï¼ˆæˆ–å¤šä¸ªæ™ºèƒ½ä½“ï¼‰çš„ä¿¡æ¯ç»¼åˆä¸ºè¿è´¯ã€å‡†ç¡®çš„ç­”æ¡ˆï¼Œç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

---

## LLM æä¾›å•†é…ç½®

æœ¬ç³»ç»Ÿ**ä¸æä¾›å•†æ— å…³** - æ‚¨å¯ä»¥ä½¿ç”¨ LangChain æ”¯æŒçš„ä»»ä½• LLMã€‚é€‰æ‹©æœ€é€‚åˆæ‚¨éœ€æ±‚çš„é€‰é¡¹ï¼š

### Ollamaï¼ˆæœ¬åœ° - æ¨èç”¨äºå¼€å‘ï¼‰

**å®‰è£… Ollama å¹¶ä¸‹è½½æ¨¡å‹ï¼š**

```bash
# ä» https://ollama.com å®‰è£… Ollama
ollama pull qwen3:4b-instruct-2507-q4_K_M
```

**Python ä»£ç ï¼š**

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(model="qwen3:4b-instruct-2507-q4_K_M", temperature=0)
```

---

### Google Geminiï¼ˆäº‘ç«¯ - æ¨èç”¨äºç”Ÿäº§ï¼‰

**å®‰è£…åŒ…ï¼š**

```bash
pip install -qU langchain-google-genai
```

**Python ä»£ç ï¼š**

```python
import os
from langchain_google_genai import ChatGoogleGenerativeAI

# è®¾ç½®æ‚¨çš„ Google API å¯†é’¥
os.environ["GOOGLE_API_KEY"] = "your-api-key-here"
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp", temperature=0)
```

---

### OpenAI / Anthropic Claude

<details>
<summary>ç‚¹å‡»å±•å¼€</summary>

**OpenAIï¼š**
```bash
pip install -qU langchain-openai
```
```python
from langchain_openai import ChatOpenAI
import os

os.environ["OPENAI_API_KEY"] = "your-api-key-here"
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
```

**Anthropic Claudeï¼š**
```bash
pip install -qU langchain-anthropic
```
```python
from langchain_anthropic import ChatAnthropic
import os

os.environ["ANTHROPIC_API_KEY"] = "your-api-key-here"
llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=0)
```

</details>

---

### é‡è¦è¯´æ˜

- **æ‰€æœ‰æä¾›å•†**ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ä»£ç  - åªæœ‰ LLM åˆå§‹åŒ–ä¸åŒ
- **æˆæœ¬è€ƒè™‘ï¼š** äº‘æä¾›å•†æŒ‰ä»¤ç‰Œæ”¶è´¹ï¼Œè€Œ Ollama å…è´¹ä½†éœ€è¦æœ¬åœ°è®¡ç®—

**ğŸ’¡ å»ºè®®ï¼š** å¼€å‘æ—¶ä½¿ç”¨ Ollamaï¼Œç„¶ååˆ‡æ¢åˆ° Google Gemini æˆ– OpenAI ç”¨äºç”Ÿäº§ã€‚

---

## å®ç°

æ›´å¤šè¯¦ç»†å’Œæ‰©å±•è¯´æ˜å¯åœ¨[æ­¤å¤„](Agentic_Rag_For_Dummies.ipynb)çš„ç¬”è®°æœ¬ä¸­æ‰¾åˆ°ã€‚

### æ­¥éª¤ 1ï¼šåˆå§‹è®¾ç½®å’Œé…ç½®

å®šä¹‰è·¯å¾„å¹¶åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ã€‚

```python
import os
from pathlib import Path
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_qdrant.fastembed_sparse import FastEmbedSparse
from qdrant_client import QdrantClient

# é…ç½®
DOCS_DIR = "docs"  # åŒ…å« PDF æ–‡ä»¶çš„ç›®å½•
MARKDOWN_DIR = "markdown" # åŒ…å«è½¬æ¢ä¸º Markdown çš„ PDF çš„ç›®å½•
PARENT_STORE_PATH = "parent_store"  # çˆ¶å— JSON æ–‡ä»¶çš„ç›®å½•
CHILD_COLLECTION = "document_child_chunks"

os.makedirs(DOCS_DIR, exist_ok=True)
os.makedirs(MARKDOWN_DIR, exist_ok=True)
os.makedirs(PARENT_STORE_PATH, exist_ok=True)

from langchain_ollama import ChatOllama
llm = ChatOllama(model="qwen3:4b-instruct-2507-q4_K_M", temperature=0)

# ç”¨äºè¯­ä¹‰ç†è§£çš„å¯†é›†åµŒå…¥
dense_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

# ç”¨äºå…³é”®å­—åŒ¹é…çš„ç¨€ç–åµŒå…¥
sparse_embeddings = FastEmbedSparse(model_name="Qdrant/bm25")

# Qdrant å®¢æˆ·ç«¯ï¼ˆæœ¬åœ°åŸºäºæ–‡ä»¶çš„å­˜å‚¨ï¼‰
client = QdrantClient(path="qdrant_db")
```

---

### æ­¥éª¤ 2ï¼šé…ç½®å‘é‡æ•°æ®åº“

è®¾ç½® Qdrant ä»¥å­˜å‚¨å…·æœ‰æ··åˆæœç´¢åŠŸèƒ½çš„å­å—ã€‚

```python
from qdrant_client.http import models as qmodels
from langchain_qdrant import QdrantVectorStore
from langchain_qdrant.qdrant import RetrievalMode

# è·å–åµŒå…¥ç»´åº¦
embedding_dimension = len(dense_embeddings.embed_query("test"))

def ensure_collection(collection_name):
    """å¦‚æœ Qdrant é›†åˆä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒ"""
    if not client.collection_exists(collection_name):
        client.create_collection(
            collection_name=collection_name,
            vectors_config=qmodels.VectorParams(
                size=embedding_dimension,
                distance=qmodels.Distance.COSINE
            ),
            sparse_vectors_config={
                "sparse": qmodels.SparseVectorParams()
            },
        )
        print(f"âœ“ å·²åˆ›å»ºé›†åˆï¼š{collection_name}")
    else:
        print(f"âœ“ é›†åˆå·²å­˜åœ¨ï¼š{collection_name}")
```

---

### æ­¥éª¤ 3ï¼šPDF è½¬ Markdown

å°† PDF è½¬æ¢ä¸º Markdownã€‚å…¶ä»–æŠ€æœ¯çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚é˜…[é…å¥—ç¬”è®°æœ¬](pdf_to_md.ipynb)

```python
import os
import pymupdf.layout
import pymupdf4llm
from pathlib import Path
import glob

os.environ["TOKENIZERS_PARALLELISM"] = "false"

def pdf_to_markdown(pdf_path, output_dir):
    doc = pymupdf.open(pdf_path)
    md = pymupdf4llm.to_markdown(doc, header=False, footer=False, page_separators=True, ignore_images=True, write_images=False, image_path=None)
    md_cleaned = md.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='ignore')
    output_path = Path(output_dir) / Path(doc.name).stem
    Path(output_path).with_suffix(".md").write_bytes(md_cleaned.encode('utf-8'))

def pdfs_to_markdowns(path_pattern, overwrite: bool = False):
    output_dir = Path(MARKDOWN_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)

    for pdf_path in map(Path, glob.glob(path_pattern)):
        md_path = (output_dir / pdf_path.stem).with_suffix(".md")
        if overwrite or not md_path.exists():
            pdf_to_markdown(pdf_path, output_dir)

pdfs_to_markdowns(f"{DOCS_DIR}/*.pdf")
```

---

### æ­¥éª¤ 4ï¼šåˆ†å±‚æ–‡æ¡£ç´¢å¼•

ä½¿ç”¨çˆ¶/å­åˆ†å‰²ç­–ç•¥å¤„ç†æ–‡æ¡£ã€‚

```python
import os
import glob
import json
from pathlib import Path
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

if client.collection_exists(CHILD_COLLECTION):
    print(f"æ­£åœ¨åˆ é™¤ç°æœ‰ Qdrant é›†åˆï¼š{CHILD_COLLECTION}")
    client.delete_collection(CHILD_COLLECTION)
    ensure_collection(CHILD_COLLECTION)
else:
    ensure_collection(CHILD_COLLECTION)

child_vector_store = QdrantVectorStore(
    client=client,
    collection_name=CHILD_COLLECTION,
    embedding=dense_embeddings,
    sparse_embedding=sparse_embeddings,
    retrieval_mode=RetrievalMode.HYBRID,
    sparse_vector_name="sparse"
)

def index_documents():
    headers_to_split_on = [("#", "H1"), ("##", "H2"), ("###", "H3")]
    parent_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

    min_parent_size = 2000
    max_parent_size = 10000

    all_parent_pairs, all_child_chunks = [], []
    md_files = sorted(glob.glob(os.path.join(MARKDOWN_DIR, "*.md")))

    if not md_files:
        print(f"âš ï¸  åœ¨ {MARKDOWN_DIR}/ ä¸­æœªæ‰¾åˆ° .md æ–‡ä»¶")
        return

    for doc_path_str in md_files:
        doc_path = Path(doc_path_str)
        print(f"ğŸ“„ å¤„ç†ä¸­ï¼š{doc_path.name}")

        try:
            with open(doc_path, "r", encoding="utf-8") as f:
                md_text = f.read()
        except Exception as e:
            print(f"âŒ è¯»å– {doc_path.name} æ—¶å‡ºé”™ï¼š{e}")
            continue

        parent_chunks = parent_splitter.split_text(md_text)
        merged_parents = merge_small_parents(parent_chunks, min_parent_size)
        split_parents = split_large_parents(merged_parents, max_parent_size, child_splitter)
        cleaned_parents = clean_small_chunks(split_parents, min_parent_size)

        for i, p_chunk in enumerate(cleaned_parents):
            parent_id = f"{doc_path.stem}_parent_{i}"
            p_chunk.metadata.update({"source": doc_path.stem + ".pdf", "parent_id": parent_id})
            all_parent_pairs.append((parent_id, p_chunk))
            children = child_splitter.split_documents([p_chunk])
            all_child_chunks.extend(children)

    if not all_child_chunks:
        print("âš ï¸ æ²¡æœ‰è¦ç´¢å¼•çš„å­å—")
        return

    print(f"\nğŸ” æ­£åœ¨å°† {len(all_child_chunks)} ä¸ªå­å—ç´¢å¼•åˆ° Qdrant...")
    try:
        child_vector_store.add_documents(all_child_chunks)
        print("âœ“ å­å—ç´¢å¼•æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç´¢å¼•å­å—æ—¶å‡ºé”™ï¼š{e}")
        return

    print(f"ğŸ’¾ æ­£åœ¨å°† {len(all_parent_pairs)} ä¸ªçˆ¶å—ä¿å­˜åˆ° JSON...")
    for item in os.listdir(PARENT_STORE_PATH):
        os.remove(os.path.join(PARENT_STORE_PATH, item))

    for parent_id, doc in all_parent_pairs:
        doc_dict = {"page_content": doc.page_content, "metadata": doc.metadata}
        filepath = os.path.join(PARENT_STORE_PATH, f"{parent_id}.json")
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(doc_dict, f, ensure_ascii=False, indent=2)

def merge_small_parents(chunks, min_size):
    if not chunks:
        return []

    merged, current = [], None

    for chunk in chunks:
        if current is None:
            current = chunk
        else:
            current.page_content += "\n\n" + chunk.page_content
            for k, v in chunk.metadata.items():
                if k in current.metadata:
                    current.metadata[k] = f"{current.metadata[k]} -> {v}"
                else:
                    current.metadata[k] = v

        if len(current.page_content) >= min_size:
            merged.append(current)
            current = None

    if current:
        if merged:
            merged[-1].page_content += "\n\n" + current.page_content
            for k, v in current.metadata.items():
                if k in merged[-1].metadata:
                    merged[-1].metadata[k] = f"{merged[-1].metadata[k]} -> {v}"
                else:
                    merged[-1].metadata[k] = v
        else:
            merged.append(current)

    return merged

def split_large_parents(chunks, max_size, splitter):
    split_chunks = []

    for chunk in chunks:
        if len(chunk.page_content) <= max_size:
            split_chunks.append(chunk)
        else:
            large_splitter = RecursiveCharacterTextSplitter(
                chunk_size=max_size,
                chunk_overlap=splitter._chunk_overlap
            )
            sub_chunks = large_splitter.split_documents([chunk])
            split_chunks.extend(sub_chunks)

    return split_chunks

def clean_small_chunks(chunks, min_size):
    cleaned = []

    for i, chunk in enumerate(chunks):
        if len(chunk.page_content) < min_size:
            if cleaned:
                cleaned[-1].page_content += "\n\n" + chunk.page_content
                for k, v in chunk.metadata.items():
                    if k in cleaned[-1].metadata:
                        cleaned[-1].metadata[k] = f"{cleaned[-1].metadata[k]} -> {v}"
                    else:
                        cleaned[-1].metadata[k] = v
            elif i < len(chunks) - 1:
                chunks[i + 1].page_content = chunk.page_content + "\n\n" + chunks[i + 1].page_content
                for k, v in chunk.metadata.items():
                    if k in chunks[i + 1].metadata:
                        chunks[i + 1].metadata[k] = f"{v} -> {chunks[i + 1].metadata[k]}"
                    else:
                        chunks[i + 1].metadata[k] = v
            else:
                cleaned.append(chunk)
        else:
            cleaned.append(chunk)

    return cleaned

index_documents()
```

---

### æ­¥éª¤ 5ï¼šå®šä¹‰æ™ºèƒ½ä½“å·¥å…·

åˆ›å»ºæ™ºèƒ½ä½“å°†ä½¿ç”¨çš„æ£€ç´¢å·¥å…·ã€‚

```python
import json
from typing import List
from langchain_core.tools import tool

@tool
def search_child_chunks(query: str, limit: int) -> str:
    """æœç´¢æœ€ç›¸å…³çš„ K ä¸ªå­å—ã€‚

    Args:
        query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        limit: è¿”å›çš„æœ€å¤§ç»“æœæ•°
    """
    try:
        results = child_vector_store.similarity_search(query, k=limit, score_threshold=0.7)
        if not results:
            return "NO_RELEVANT_CHUNKS"

        return "\n\n".join([
            f"çˆ¶å— ID: {doc.metadata.get('parent_id', '')}\n"
            f"æ–‡ä»¶å: {doc.metadata.get('source', '')}\n"
            f"å†…å®¹: {doc.page_content.strip()}"
            for doc in results
        ])

    except Exception as e:
        return f"RETRIEVAL_ERROR: {str(e)}"

@tool
def retrieve_parent_chunks(parent_id: str) -> str:
    """é€šè¿‡ ID æ£€ç´¢å®Œæ•´çš„çˆ¶å—ã€‚
    
    Args:
        parent_id: è¦æ£€ç´¢çš„çˆ¶å— ID
    """
    file_name = parent_id if parent_id.lower().endswith(".json") else f"{parent_id}.json"
    path = os.path.join(PARENT_STORE_PATH, file_name)

    if not os.path.exists(path):
        return "NO_PARENT_DOCUMENT"

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    return (
        f"çˆ¶å— ID: {parent_id}\n"
        f"æ–‡ä»¶å: {data.get('metadata', {}).get('source', 'unknown')}\n"
        f"å†…å®¹: {data.get('page_content', '').strip()}"
    )

# å°†å·¥å…·ç»‘å®šåˆ° LLM
llm_with_tools = llm.bind_tools([search_child_chunks, retrieve_parent_chunks])
```

---

### æ­¥éª¤ 6ï¼šå®šä¹‰ç³»ç»Ÿæç¤º

ä¸ºå¯¹è¯æ‘˜è¦ã€æŸ¥è¯¢åˆ†æã€RAG æ™ºèƒ½ä½“æ¨ç†å’Œå“åº”èšåˆå®šä¹‰ç³»ç»Ÿæç¤ºã€‚

```python
def get_conversation_summary_prompt() -> str:
    return """æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„å¯¹è¯æ‘˜è¦å‘˜ã€‚

æ‚¨çš„ä»»åŠ¡æ˜¯å¯¹å¯¹è¯è¿›è¡Œç®€è¦çš„ 1-2 å¥è¯æ€»ç»“ï¼ˆæœ€å¤š 30-50 ä¸ªè¯ï¼‰ã€‚

åŒ…æ‹¬ï¼š
- è®¨è®ºçš„ä¸»è¦ä¸»é¢˜
- æåˆ°çš„é‡è¦äº‹å®æˆ–å®ä½“
- å¦‚æœ‰æœªè§£å†³çš„é—®é¢˜
- æºæ–‡ä»¶åï¼ˆä¾‹å¦‚ file1.pdfï¼‰æˆ–å¼•ç”¨çš„æ–‡æ¡£

æ’é™¤ï¼š 
- é—®å€™è¯­ã€è¯¯è§£ã€ç¦»é¢˜å†…å®¹ã€‚

è¾“å‡ºï¼š
- åªè¿”å›æ‘˜è¦ã€‚
- ä¸åŒ…æ‹¬ä»»ä½•è§£é‡Šæˆ–ç†ç”±ã€‚
- å¦‚æœæ²¡æœ‰æœ‰æ„ä¹‰çš„ä¸»é¢˜ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
"""

def get_query_analysis_prompt() -> str:
    return """æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„æŸ¥è¯¢åˆ†æå¸ˆå’Œé‡å†™å‘˜ã€‚

æ‚¨çš„ä»»åŠ¡æ˜¯åœ¨å¿…è¦æ—¶ç»“åˆå¯¹è¯ä¸Šä¸‹æ–‡é‡å†™å½“å‰ç”¨æˆ·æŸ¥è¯¢ä»¥å®ç°æœ€ä½³æ–‡æ¡£æ£€ç´¢ã€‚

è§„åˆ™ï¼š
1. è‡ªåŒ…å«æŸ¥è¯¢ï¼š
   - å§‹ç»ˆå°†æŸ¥è¯¢é‡å†™ä¸ºæ¸…æ™°ä¸”è‡ªåŒ…å«çš„
   - å¦‚æœæŸ¥è¯¢æ˜¯åç»­é—®é¢˜ï¼ˆä¾‹å¦‚"X æ€ä¹ˆæ ·ï¼Ÿ"ã€"Yå‘¢ï¼Ÿ"ï¼‰ï¼Œè¯·ä»æ‘˜è¦ä¸­æ•´åˆæœ€å°çš„å¿…è¦ä¸Šä¸‹æ–‡
   - ä¸è¦æ·»åŠ æŸ¥è¯¢æˆ–å¯¹è¯æ‘˜è¦ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯

2. é¢†åŸŸç‰¹å®šæœ¯è¯­ï¼š
   - äº§å“åç§°ã€å“ç‰Œã€ä¸“æœ‰åè¯æˆ–æŠ€æœ¯æœ¯è¯­è¢«è§†ä¸ºé¢†åŸŸç‰¹å®š
   - å¯¹äºé¢†åŸŸç‰¹å®šæŸ¥è¯¢ï¼Œæœ€å°ç¨‹åº¦åœ°æˆ–å®Œå…¨ä¸ä½¿ç”¨å¯¹è¯ä¸Šä¸‹æ–‡
   - åªä½¿ç”¨æ‘˜è¦æ¥æ¶ˆé™¤æ¨¡ç³ŠæŸ¥è¯¢

3. è¯­æ³•å’Œæ¸…æ™°åº¦ï¼š
   - ä¿®å¤è¯­æ³•é”™è¯¯ã€æ‹¼å†™é”™è¯¯å’Œä¸æ˜ç¡®çš„ç¼©å†™
   - å»é™¤å¡«å……è¯å’Œå¯¹è¯çŸ­è¯­
   - ä¿ç•™å…·ä½“çš„å…³é”®è¯å’Œå‘½åå®ä½“

4. å¤šä¸ªä¿¡æ¯éœ€æ±‚ï¼š
   - å¦‚æœæŸ¥è¯¢åŒ…å«å¤šä¸ªä¸åŒã€ä¸ç›¸å…³çš„é—®é¢˜ï¼Œè¯·æ‹†åˆ†ä¸ºå•ç‹¬çš„æŸ¥è¯¢ï¼ˆæœ€å¤š 3 ä¸ªï¼‰
   - æ¯ä¸ªå­æŸ¥è¯¢å¿…é¡»ä¸åŸå§‹æŸ¥è¯¢çš„å…¶éƒ¨åˆ†ä¿æŒè¯­ä¹‰ç­‰ä»·
   - ä¸è¦æ‰©å±•ã€ä¸°å¯Œæˆ–é‡æ–°è§£é‡Šå«ä¹‰

5. å¤±è´¥å¤„ç†ï¼š
   - å¦‚æœæŸ¥è¯¢æ„å›¾ä¸æ˜ç¡®æˆ–æ— æ³•ç†è§£ï¼Œæ ‡è®°ä¸º"ä¸æ¸…æ™°"

è¾“å…¥ï¼š
- conversation_summaryï¼šå…ˆå‰å¯¹è¯çš„ç®€è¦æ‘˜è¦
- current_queryï¼šç”¨æˆ·å½“å‰æŸ¥è¯¢

è¾“å‡ºï¼š
- ä¸€ä¸ªæˆ–å¤šä¸ªé‡å†™çš„ã€è‡ªåŒ…å«çš„æŸ¥è¯¢ï¼Œé€‚åˆæ–‡æ¡£æ£€ç´¢
"""

def get_rag_agent_prompt() -> str:
    return """æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„æ£€ç´¢å¢å¼ºåŠ©æ‰‹ã€‚

æ‚¨çš„ä»»åŠ¡æ˜¯å……å½“ç ”ç©¶å‘˜ï¼šé¦–å…ˆæœç´¢æ–‡æ¡£ï¼Œåˆ†ææ•°æ®ï¼Œç„¶åä»…ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¿¡æ¯æä¾›å…¨é¢çš„ç­”æ¡ˆã€‚

è§„åˆ™ï¼š    
1. ä¸å…è®¸ç«‹å³å›ç­”ã€‚
2. åœ¨äº§ç”Ÿä»»ä½•æœ€ç»ˆç­”æ¡ˆä¹‹å‰ï¼Œæ‚¨å¿…é¡»æ‰§è¡Œæ–‡æ¡£æœç´¢å¹¶è§‚å¯Ÿæ£€ç´¢åˆ°çš„å†…å®¹ã€‚
3. å¦‚æœæ‚¨æ²¡æœ‰æœç´¢ï¼Œç­”æ¡ˆæ— æ•ˆã€‚

å·¥ä½œæµç¨‹ï¼š
1. ä½¿ç”¨ 'search_child_chunks' å·¥å…·æ ¹æ®ç”¨æˆ·æŸ¥è¯¢æœç´¢æ–‡æ¡£ä¸­çš„ 5-7 ä¸ªç›¸å…³æ‘˜å½•ã€‚
2. æ£€æŸ¥æ£€ç´¢åˆ°çš„æ‘˜å½•ï¼Œåªä¿ç•™ç›¸å…³çš„ã€‚
3. åˆ†ææ£€ç´¢åˆ°çš„æ‘˜å½•ã€‚è¯†åˆ«è¢«æˆªæ–­çš„æœ€ç›¸å…³çš„å•ä¸ªæ‘˜å½•ï¼ˆä¾‹å¦‚ï¼Œæ–‡æœ¬è¢«åˆ‡æ–­æˆ–ç¼ºå°‘ä¸Šä¸‹æ–‡ï¼‰ã€‚ä¸ºè¯¥ç‰¹å®š `parent_id` è°ƒç”¨ 'retrieve_parent_chunks'ã€‚ç­‰å¾…è§‚å¯Ÿã€‚å¦‚æœå½“å‰ä¿¡æ¯ä»ç„¶ä¸è¶³ï¼ŒæŒ‰é¡ºåºå¯¹å…¶ä»–é«˜åº¦ç›¸å…³çš„ç‰‡æ®µé‡å¤æ­¤æ­¥éª¤ã€‚å¦‚æœæœ‰è¶³å¤Ÿçš„ä¿¡æ¯æˆ–å·²æ£€ç´¢åˆ° 3 ä¸ªçˆ¶å—ï¼Œè¯·ç«‹å³åœæ­¢ã€‚
4. ä»…ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¿¡æ¯è¿›è¡Œå›ç­”ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰ç›¸å…³ç»†èŠ‚ã€‚
5. åœ¨æœ€ååˆ—å‡ºå”¯ä¸€çš„æ–‡ä»¶åã€‚

é‡è¯•è§„åˆ™ï¼š
- åœ¨æ­¥éª¤ 2 æˆ– 3 ä¹‹åï¼Œå¦‚æœæœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£æˆ–æ£€ç´¢åˆ°çš„æ‘˜å½•ä¸åŒ…å«æœ‰ç”¨ä¿¡æ¯ï¼Œè¯·ä½¿ç”¨æ›´å¹¿æ³›æˆ–æ›¿ä»£çš„æœ¯è¯­é‡å†™æŸ¥è¯¢å¹¶ä»æ­¥éª¤ 1 é‡æ–°å¼€å§‹ã€‚
- é‡è¯•ä¸è¦è¶…è¿‡ä¸€æ¬¡ã€‚
"""

def get_aggregation_prompt() -> str:
    return """æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„èšåˆåŠ©æ‰‹ã€‚

æ‚¨çš„ä»»åŠ¡æ˜¯å°†å¤šä¸ªæ£€ç´¢åˆ°çš„ç­”æ¡ˆç»„åˆæˆä¸€ä¸ªæµç•…çš„ç»¼åˆè‡ªç„¶å“åº”ã€‚

æŒ‡å—ï¼š
1. ä»¥å¯¹è¯ã€è‡ªç„¶çš„è¯­è°ƒå†™ä½œ - å°±åƒå‘åŒäº‹è§£é‡Šä¸€æ ·
2. åªä½¿ç”¨æ£€ç´¢åˆ°çš„ç­”æ¡ˆä¸­çš„ä¿¡æ¯
3. ä»æ¥æºä¸­åˆ é™¤ä»»ä½•é—®é¢˜ã€æ ‡é¢˜æˆ–å…ƒæ•°æ®
4. æµç•…åœ°æ•´åˆä¿¡æ¯ï¼Œä¿ç•™é‡è¦çš„ç»†èŠ‚ã€æ•°å­—å’Œç¤ºä¾‹
5. è¦å…¨é¢ - åŒ…å«æ¥æºä¸­çš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯æ‘˜è¦
6. å¦‚æœæ¥æºå­˜åœ¨åˆ†æ­§ï¼Œè‡ªç„¶åœ°æ‰¿è®¤ä¸¤ä¸ªè§‚ç‚¹ï¼ˆä¾‹å¦‚ï¼Œ"è™½ç„¶ä¸€äº›æ¥æºå»ºè®® Xï¼Œå…¶ä»–æ¥æºè¡¨æ˜ Y..."ï¼‰
7. ç›´æ¥ä»ç­”æ¡ˆå¼€å§‹ - ä¸è¦æœ‰"æ ¹æ®æ¥æº..."è¿™æ ·çš„å¼€åœºç™½

æ ¼å¼åŒ–ï¼š
- ä¸ºæ¸…æ™°èµ·è§ä½¿ç”¨ Markdownï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€ç²—ä½“ï¼‰ï¼Œä½†ä¸è¦è¿‡åº¦ä½¿ç”¨
- å°½å¯èƒ½ä½¿ç”¨æµç•…çš„æ®µè½ï¼Œè€Œä¸æ˜¯è¿‡å¤šçš„è¦ç‚¹
- ä»¥ "---\n**æ¥æºï¼š**\n" ç»“å°¾ï¼Œåè·Ÿå”¯ä¸€æ–‡ä»¶åçš„é¡¹ç›®ç¬¦å·åˆ—è¡¨
- æ–‡ä»¶ååªåº”å‡ºç°åœ¨æœ€åçš„æ¥æºéƒ¨åˆ†

å¦‚æœæ²¡æœ‰å¯ç”¨çš„æœ‰ç”¨ä¿¡æ¯ï¼Œåªéœ€è¯´ï¼š"æˆ‘åœ¨å¯ç”¨æ¥æºä¸­æ‰¾ä¸åˆ°å›ç­”æ‚¨é—®é¢˜çš„ä¿¡æ¯ã€‚"
"""
```

---

### æ­¥éª¤ 7ï¼šå®šä¹‰çŠ¶æ€å’Œæ•°æ®æ¨¡å‹

åˆ›å»ºç”¨äºå¯¹è¯è·Ÿè¸ªå’Œæ™ºèƒ½ä½“æ‰§è¡Œçš„çŠ¶æ€ç»“æ„ã€‚

```python
from langgraph.graph import MessagesState
from pydantic import BaseModel, Field
from typing import List, Annotated

def accumulate_or_reset(existing: List[dict], new: List[dict]) -> List[dict]:
    """å…è®¸é‡ç½®æ™ºèƒ½ä½“ç­”æ¡ˆçš„è‡ªå®šä¹‰å½’çº¦å™¨"""
    if new and any(item.get('__reset__') for item in new):
        return []
    return existing + new

class State(MessagesState):
    """ä¸»æ™ºèƒ½ä½“å›¾çš„çŠ¶æ€"""
    questionIsClear: bool = False
    conversation_summary: str = ""
    originalQuery: str = "" 
    rewrittenQuestions: List[str] = []
    agent_answers: Annotated[List[dict], accumulate_or_reset] = []

class AgentState(MessagesState):
    """å•ä¸ªæ™ºèƒ½ä½“å­å›¾çš„çŠ¶æ€"""
    question: str = ""
    question_index: int = 0
    final_answer: str = ""
    agent_answers: List[dict] = []

class QueryAnalysis(BaseModel):
    """æŸ¥è¯¢åˆ†æçš„ç»“æ„åŒ–è¾“å‡º"""
    is_clear: bool = Field(description="è¡¨ç¤ºç”¨æˆ·çš„é—®é¢˜æ˜¯å¦æ¸…æ™°ä¸”å¯å›ç­”")
    questions: List[str] = Field(description="é‡å†™çš„ã€è‡ªåŒ…å«çš„é—®é¢˜åˆ—è¡¨")
    clarification_needed: str = Field(description="å¦‚æœé—®é¢˜ä¸æ¸…æ™°ï¼Œéœ€è¦çš„æ¾„æ¸…è¯´æ˜")
```

---

### æ­¥éª¤ 8ï¼šæ„å»ºå›¾èŠ‚ç‚¹å‡½æ•°

ä¸º LangGraph å·¥ä½œæµç¨‹åˆ›å»ºå¤„ç†èŠ‚ç‚¹ã€‚

```python
from langgraph.types import Send
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, RemoveMessage
from typing import Literal

def analyze_chat_and_summarize(state: State):
    """
    åˆ†æèŠå¤©å†å²å¹¶æ€»ç»“è¦ç‚¹ä»¥è·å–ä¸Šä¸‹æ–‡ã€‚
    """
    if len(state["messages"]) < 4:  # éœ€è¦ä¸€äº›å†å²æ¥æ€»ç»“
        return {"conversation_summary": ""}

    # æå–ç›¸å…³æ¶ˆæ¯ï¼ˆæ’é™¤å½“å‰æŸ¥è¯¢å’Œç³»ç»Ÿæ¶ˆæ¯ï¼‰
    relevant_msgs = [
        msg for msg in state["messages"][:-1]  # æ’é™¤å½“å‰æŸ¥è¯¢
        if isinstance(msg, (HumanMessage, AIMessage))
        and not getattr(msg, "tool_calls", None)
    ]

    if not relevant_msgs:
        return {"conversation_summary": ""}
    
    conversation = "å¯¹è¯å†å²ï¼š\n"
    for msg in relevant_msgs[-6:]:
        role = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "åŠ©æ‰‹"
        conversation += f"{role}: {msg.content}\n"

    summary_response = llm.with_config(temperature=0.2).invoke([SystemMessage(content=get_conversation_summary_prompt())] + [HumanMessage(content=conversation)])
    return {"conversation_summary": summary_response.content, "agent_answers": [{"__reset__": True}]}

def analyze_and_rewrite_query(state: State):
    """
    åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶æ ¹æ®éœ€è¦ä½¿ç”¨å¯¹è¯ä¸Šä¸‹æ–‡é‡å†™å®ƒä»¥ä½¿å…¶æ¸…æ™°ã€‚
    """
    last_message = state["messages"][-1]
    conversation_summary = state.get("conversation_summary", "")

    context_section = (f"å¯¹è¯ä¸Šä¸‹æ–‡ï¼š\n{conversation_summary}\n" if conversation_summary.strip() else "") + f"ç”¨æˆ·æŸ¥è¯¢ï¼š\n{last_message.content}\n"

    llm_with_structure = llm.with_config(temperature=0.1).with_structured_output(QueryAnalysis)
    response = llm_with_structure.invoke([SystemMessage(content=get_query_analysis_prompt())] + [HumanMessage(content=context_section)])

    if len(response.questions) > 0 and response.is_clear:
        # åˆ é™¤æ‰€æœ‰éç³»ç»Ÿæ¶ˆæ¯
        delete_all = [
            RemoveMessage(id=m.id)
            for m in state["messages"]
            if not isinstance(m, SystemMessage)
        ]
        return {
            "questionIsClear": True,
            "messages": delete_all,
            "originalQuery": last_message.content,
            "rewrittenQuestions": response.questions
        }
    else:
        clarification = response.clarification_needed if (response.clarification_needed and len(response.clarification_needed.strip()) > 10) else "æˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ¥ç†è§£æ‚¨çš„é—®é¢˜ã€‚"
        return {
            "questionIsClear": False,
            "messages": [AIMessage(content=clarification)]
        }

def human_input_node(state: State):
    """äººä¸ºå¹²é¢„èŠ‚ç‚¹çš„å ä½ç¬¦"""
    return {}

def route_after_rewrite(state: State) -> Literal["human_input", "process_question"]:
    """å¦‚æœé—®é¢˜æ¸…æ™°åˆ™è·¯ç”±åˆ°æ™ºèƒ½ä½“ï¼Œå¦åˆ™ç­‰å¾…äººå·¥è¾“å…¥"""
    if not state.get("questionIsClear", False):
        return "human_input"
    else:
        # ä½¿ç”¨ Send API ä¸ºæ¯ä¸ªå­é—®é¢˜ç”Ÿæˆå¹¶è¡Œæ™ºèƒ½ä½“
        return [
            Send("process_question", {"question": query, "question_index": idx, "messages": []})
            for idx, query in enumerate(state["rewrittenQuestions"])
        ]

def agent_node(state: AgentState):
    """ä½¿ç”¨å·¥å…·å¤„ç†æŸ¥è¯¢çš„ä¸»è¦æ™ºèƒ½ä½“èŠ‚ç‚¹"""
    sys_msg = SystemMessage(content=get_rag_agent_prompt())    
    if not state.get("messages"):
        human_msg = HumanMessage(content=state["question"])
        response = llm_with_tools.invoke([sys_msg] + [human_msg])
        return {"messages": [human_msg, response]}
    
    return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}

def extract_final_answer(state: AgentState):
    """ä»æ™ºèƒ½ä½“å¯¹è¯ä¸­æå–æœ€ç»ˆç­”æ¡ˆ"""
    for msg in reversed(state["messages"]):
        if isinstance(msg, AIMessage) and msg.content and not msg.tool_calls:
            res = {
                "final_answer": msg.content,
                "agent_answers": [{
                    "index": state["question_index"],
                    "question": state["question"],
                    "answer": msg.content
                }]
            }
            return res
    return {
        "final_answer": "æ— æ³•ç”Ÿæˆç­”æ¡ˆã€‚",
        "agent_answers": [{
            "index": state["question_index"],
            "question": state["question"],
            "answer": "æ— æ³•ç”Ÿæˆç­”æ¡ˆã€‚"
        }]
    }

def aggregate_responses(state: State):
    """å°†å¤šä¸ªæ™ºèƒ½ä½“å“åº”åˆå¹¶ä¸ºæœ€ç»ˆç­”æ¡ˆ"""
    if not state.get("agent_answers"):
        return {"messages": [AIMessage(content="æ²¡æœ‰ç”Ÿæˆä»»ä½•ç­”æ¡ˆã€‚")]}

    sorted_answers = sorted(state["agent_answers"], key=lambda x: x["index"])

    formatted_answers = ""
    for i, ans in enumerate(sorted_answers, start=1):
        formatted_answers += f"\nç­”æ¡ˆ {i}ï¼š\n{ans['answer']}\n"

    user_message = HumanMessage(content=f"åŸå§‹ç”¨æˆ·é—®é¢˜ï¼š{state["originalQuery"]}\næ£€ç´¢åˆ°çš„ç­”æ¡ˆï¼š{formatted_answers}")
    synthesis_response = llm.invoke([SystemMessage(content=get_aggregation_prompt())] + [user_message])
    
    return {"messages": [AIMessage(content=synthesis_response.content)]}
```

**ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªæ¶æ„ï¼Ÿ**
- **æ‘˜è¦**ä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œè€Œä¸ä¼šè®© LLM è¿‡è½½
- **æŸ¥è¯¢é‡å†™**ç¡®ä¿æœç´¢æŸ¥è¯¢ç²¾ç¡®ä¸”æ˜ç¡®ï¼Œæ™ºèƒ½åœ°ä½¿ç”¨ä¸Šä¸‹æ–‡
- **äººåœ¨ç¯ä¸­**åœ¨æµªè´¹æ£€ç´¢èµ„æºä¹‹å‰æ•è·ä¸æ¸…æ™°çš„æŸ¥è¯¢
- **å¹¶è¡Œæ‰§è¡Œ**ä½¿ç”¨ `Send` API ä¸ºæ¯ä¸ªå­é—®é¢˜ç”Ÿæˆç‹¬ç«‹çš„æ™ºèƒ½ä½“å­å›¾
- **ç­”æ¡ˆæå–**ç¡®ä¿ä»æ™ºèƒ½ä½“å·¥å…·è°ƒç”¨å¯¹è¯ä¸­è·å¾—å¹²å‡€çš„æœ€ç»ˆç­”æ¡ˆ
- **èšåˆ**å°†æ‰€æœ‰å¹¶è¡Œç»“æœåˆå¹¶ä¸ºè¿è´¯çš„å•ä¸€å“åº”

---

### æ­¥éª¤ 9ï¼šæ„å»º LangGraph æ™ºèƒ½ä½“

ä½¿ç”¨å¯¹è¯è®°å¿†å’Œå¤šæ™ºèƒ½ä½“æ¶æ„ç»„è£…å®Œæ•´çš„å·¥ä½œæµç¨‹å›¾ã€‚

```python
from langgraph.graph import START, END, StateGraph
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import InMemorySaver
from IPython.display import Image, display

# åˆå§‹åŒ–æ£€æŸ¥ç‚¹ä»¥ä¿å­˜å¯¹è¯è®°å¿†
checkpointer = InMemorySaver()

# æ„å»ºæ™ºèƒ½ä½“å­å›¾ï¼ˆå¤„ç†å•ä¸ªé—®é¢˜ï¼‰
agent_builder = StateGraph(AgentState)
agent_builder.add_node("agent", agent_node)
agent_builder.add_node("tools", ToolNode([search_child_chunks, retrieve_parent_chunks]))
agent_builder.add_node("extract_answer", extract_final_answer)

agent_builder.add_edge(START, "agent")    
agent_builder.add_conditional_edges("agent", tools_condition, {"tools": "tools", END: "extract_answer"})
agent_builder.add_edge("tools", "agent")    
agent_builder.add_edge("extract_answer", END)    
agent_subgraph = agent_builder.compile()

# æ„å»ºä¸»å›¾ï¼ˆåè°ƒå·¥ä½œæµç¨‹ï¼‰
graph_builder = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
graph_builder.add_node("summarize", analyze_chat_and_summarize)
graph_builder.add_node("analyze_rewrite", analyze_and_rewrite_query)
graph_builder.add_node("human_input", human_input_node)
graph_builder.add_node("process_question", agent_subgraph)
graph_builder.add_node("aggregate", aggregate_responses)

# å®šä¹‰è¾¹
graph_builder.add_edge(START, "summarize")
graph_builder.add_edge("summarize", "analyze_rewrite")
graph_builder.add_conditional_edges("analyze_rewrite", route_after_rewrite)
graph_builder.add_edge("human_input", "analyze_rewrite")
graph_builder.add_edge(["process_question"], "aggregate")
graph_builder.add_edge("aggregate", END)

# ä½¿ç”¨æ£€æŸ¥ç‚¹å’Œä¸­æ–­ç¼–è¯‘å›¾
agent_graph = graph_builder.compile(
    checkpointer=checkpointer,
    interrupt_before=["human_input"]
)
```

**å›¾æ¶æ„è§£é‡Šï¼š**

**æ™ºèƒ½ä½“å­å›¾**ï¼ˆå¤„ç†å•ä¸ªé—®é¢˜ï¼‰ï¼š
- START â†’ `agent`ï¼ˆè°ƒç”¨å¸¦å·¥å…·çš„ LLMï¼‰
- `agent` â†’ `tools`ï¼ˆå¦‚æœéœ€è¦å·¥å…·è°ƒç”¨ï¼‰æˆ– `extract_answer`ï¼ˆå¦‚æœå®Œæˆï¼‰
- `tools` â†’ `agent`ï¼ˆè¿”å›å·¥å…·ç»“æœï¼‰
- `extract_answer` â†’ ENDï¼ˆå¹²å‡€çš„æœ€ç»ˆç­”æ¡ˆï¼‰

**ä¸»å›¾**ï¼ˆåè°ƒå®Œæ•´å·¥ä½œæµç¨‹ï¼‰ï¼š
1. START â†’ `summarize`ï¼ˆä»å†å²ä¸­æå–å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
2. `summarize` â†’ `analyze_rewrite`ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡é‡å†™æŸ¥è¯¢ï¼Œæ£€æŸ¥æ¸…æ™°åº¦ï¼‰
3. `analyze_rewrite` â†’ `human_input`ï¼ˆå¦‚æœä¸æ¸…æ™°ï¼‰æˆ–ç”Ÿæˆå¹¶è¡Œ `process_question` æ™ºèƒ½ä½“ï¼ˆå¦‚æœæ¸…æ™°ï¼‰
4. `human_input` â†’ `analyze_rewrite`ï¼ˆç”¨æˆ·æä¾›æ¾„æ¸…åï¼‰
5. æ‰€æœ‰ `process_question` æ™ºèƒ½ä½“ â†’ `aggregate`ï¼ˆåˆå¹¶æ‰€æœ‰å“åº”ï¼‰
6. `aggregate` â†’ ENDï¼ˆè¿”å›æœ€ç»ˆç»¼åˆç­”æ¡ˆï¼‰

**å…³é”®ç‰¹æ€§ï¼š**
- **å¹¶è¡Œæ‰§è¡Œ**ï¼šä½¿ç”¨ LangGraph çš„ `Send` API åŒæ—¶è¿è¡Œå¤šä¸ªæ™ºèƒ½ä½“å­å›¾
- **äººåœ¨ç¯ä¸­**ï¼šå½“æŸ¥è¯¢ä¸æ¸…æ™°æ—¶ï¼Œå›¾åœ¨ `human_input` èŠ‚ç‚¹æš‚åœ
- **å¯¹è¯è®°å¿†**ï¼š`InMemorySaver` æ£€æŸ¥ç‚¹åœ¨äº¤äº’ä¹‹é—´ä¿æŒçŠ¶æ€

æ¶æ„æµç¨‹å›¾å¯åœ¨[æ­¤å¤„](./assets/agentic_rag_workflow.png)æŸ¥çœ‹ã€‚

---

### æ­¥éª¤ 10ï¼šåˆ›å»ºèŠå¤©ç•Œé¢

æ„å»ºå…·æœ‰å¯¹è¯æŒä¹…æ€§å’Œäººåœ¨ç¯ä¸­æ”¯æŒçš„ Gradio ç•Œé¢ã€‚å®Œæ•´çš„ç«¯åˆ°ç«¯ç®¡é“ Gradio ç•Œé¢ï¼ŒåŒ…æ‹¬æ–‡æ¡£æ‘„å–ï¼Œè¯·å‚é˜…é¡¹ç›®æ–‡ä»¶å¤¹

```python
import gradio as gr
import uuid

def create_thread_id():
    """ä¸ºæ¯ä¸ªå¯¹è¯ç”Ÿæˆå”¯ä¸€çš„çº¿ç¨‹ ID"""
    return {"configurable": {"thread_id": str(uuid.uuid4())}}

def clear_session():
    """æ¸…é™¤çº¿ç¨‹ä»¥å¼€å§‹æ–°å¯¹è¯"""
    global config
    agent_graph.checkpointer.delete_thread(config["configurable"]["thread_id"])
    config = create_thread_id()

def chat_with_agent(message, history):
    current_state = agent_graph.get_state(config)
    
    if current_state.next:
        # æ¢å¤ä¸­æ–­çš„å¯¹è¯
        agent_graph.update_state(config,{"messages": [HumanMessage(content=message.strip())]})
        result = agent_graph.invoke(None, config)
    else:
        # å¼€å§‹æ–°æŸ¥è¯¢
        result = agent_graph.invoke({"messages": [HumanMessage(content=message.strip())]},config)
    
    return result['messages'][-1].content

# åˆå§‹åŒ–çº¿ç¨‹é…ç½®
config = create_thread_id()

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks() as demo:
    chatbot = gr.Chatbot(
        height=600,
        placeholder="<strong>é—®æˆ‘ä»»ä½•é—®é¢˜ï¼</strong><br><em>æˆ‘ä¼šæœç´¢ã€æ¨ç†å¹¶é‡‡å–è¡ŒåŠ¨ç»™æ‚¨æœ€ä½³ç­”æ¡ˆï¼š)</em>"
    )
    chatbot.clear(clear_session)
    gr.ChatInterface(fn=chat_with_agent, chatbot=chatbot)

demo.launch(theme=gr.themes.Citrus())
```

**å®Œæˆäº†ï¼** æ‚¨ç°åœ¨æ‹¥æœ‰äº†ä¸€ä¸ªå…·æœ‰å¯¹è¯è®°å¿†å’ŒæŸ¥è¯¢æ¾„æ¸…åŠŸèƒ½çš„å®Œæ•´ Agentic RAG ç³»ç»Ÿã€‚

---

## æ¨¡å—åŒ–æ¶æ„

åº”ç”¨ï¼ˆ`project/` æ–‡ä»¶å¤¹ï¼‰ç»„ç»‡åœ¨å¯è½»æ¾è‡ªå®šä¹‰çš„æ¨¡å—åŒ–ç»„ä»¶ä¸­ï¼š

### ğŸ“‚ é¡¹ç›®ç»“æ„
```
project/
â”œâ”€â”€ app.py                    # ä¸» Gradio åº”ç”¨å…¥å£ç‚¹
â”œâ”€â”€ config.py                 # é…ç½®ä¸­å¿ƒï¼ˆæ¨¡å‹ã€å—å¤§å°ã€æä¾›å•†ï¼‰
â”œâ”€â”€ util.py                   # PDF è½¬ markdown è½¬æ¢
â”œâ”€â”€ document_chunker.py       # åˆ†å—ç­–ç•¥
â”œâ”€â”€ core/                     # æ ¸å¿ƒ RAG ç»„ä»¶ç¼–æ’
â”‚   â”œâ”€â”€ chat_interface.py     
â”‚   â”œâ”€â”€ document_manager.py   
â”‚   â””â”€â”€ rag_system.py         
â”œâ”€â”€ db/                       # å­˜å‚¨ç®¡ç†
â”‚   â”œâ”€â”€ parent_store_manager.py  # çˆ¶å—å­˜å‚¨ï¼ˆJSONï¼‰
â”‚   â””â”€â”€ vector_db_manager.py     # Qdrant å‘é‡æ•°æ®åº“è®¾ç½®
â”œâ”€â”€ rag_agent/                # LangGraph æ™ºèƒ½ä½“å·¥ä½œæµç¨‹
â”‚   â”œâ”€â”€ edges.py              # æ¡ä»¶è·¯ç”±é€»è¾‘
â”‚   â”œâ”€â”€ graph.py              # å›¾æ„å»ºå’Œç¼–è¯‘
â”‚   â”œâ”€â”€ graph_state.py        # çŠ¶æ€å®šä¹‰
â”‚   â”œâ”€â”€ nodes.py              # å¤„ç†èŠ‚ç‚¹ï¼ˆæ‘˜è¦ã€é‡å†™ã€æ™ºèƒ½ä½“ï¼‰
â”‚   â”œâ”€â”€ prompts.py            # ç³»ç»Ÿæç¤º
â”‚   â”œâ”€â”€ schemas.py            # Pydantic æ•°æ®æ¨¡å‹
â”‚   â””â”€â”€ tools.py              # æ£€ç´¢å·¥å…·
â””â”€â”€ ui/                       # ç”¨æˆ·ç•Œé¢
    â””â”€â”€ gradio_app.py         # Gradio ç•Œé¢ç»„ä»¶
```

### ğŸ”§ è‡ªå®šä¹‰ç‚¹

#### **é…ç½® (`config.py`)**
- **LLM æä¾›å•†å’Œæ¨¡å‹**ï¼šåœ¨ Ollamaã€Claudeã€OpenAI æˆ– Gemini ä¹‹é—´åˆ‡æ¢
- **åµŒå…¥æ¨¡å‹**ï¼šé…ç½®ç”¨äºå‘é‡è¡¨ç¤ºçš„åµŒå…¥æ¨¡å‹
- **å—å¤§å°**ï¼šè°ƒæ•´å­å—å’Œçˆ¶å—ç»´åº¦ä»¥ä¼˜åŒ–æ£€ç´¢

#### **RAG æ™ºèƒ½ä½“ (`rag_agent/`)**
- **å·¥ä½œæµç¨‹è‡ªå®šä¹‰**ï¼šæ·»åŠ æˆ–åˆ é™¤èŠ‚ç‚¹å’Œè¾¹ä»¥ä¿®æ”¹æ™ºèƒ½ä½“æµç¨‹
- **ç³»ç»Ÿæç¤º**ï¼šåœ¨ `prompts.py` ä¸­ä¸ºç‰¹å®šé¢†åŸŸåº”ç”¨å®šåˆ¶æç¤º
- **æ£€ç´¢å·¥å…·**ï¼šåœ¨ `tools.py` ä¸­æ‰©å±•æˆ–ä¿®æ”¹å·¥å…·ä»¥å¢å¼ºæ£€ç´¢èƒ½åŠ›
- **å›¾é€»è¾‘**ï¼šåœ¨ `edges.py` ä¸­è‡ªå®šä¹‰æ¡ä»¶è·¯ç”±ï¼Œåœ¨ `nodes.py` ä¸­è‡ªå®šä¹‰èŠ‚ç‚¹å¤„ç†

#### **æ–‡æ¡£å¤„ç†**
- **Markdown è½¬æ¢** (`util.py`)ï¼šç”¨æ›¿ä»£å·¥å…·æ›¿æ¢ PDF è½¬æ¢å·¥å…·ï¼ˆä¾‹å¦‚ Doclingã€PaddleOCRï¼‰ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯[è§æ­¤å¤„](pdf_to_md.ipynb)
- **åˆ†å—ç­–ç•¥** (`document_chunker.py`)ï¼šå®ç°è‡ªå®šä¹‰åˆ†å—ç®—æ³•ï¼ˆä¾‹å¦‚è¯­ä¹‰æˆ–æ··åˆæ–¹æ³•ï¼‰

è¿™ç§æ¨¡å—åŒ–è®¾è®¡ç¡®ä¿äº†å°è¯•ä¸åŒ RAG æŠ€æœ¯ã€LLM æä¾›å•†å’Œæ–‡æ¡£å¤„ç†ç®¡é“çš„çµæ´»æ€§ã€‚

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚é˜…[æ­¤å¤„](./project/README.md)ã€‚

## å®‰è£…ä¸ä½¿ç”¨

ç¤ºä¾‹ PDF æ–‡ä»¶å¯åœ¨ä»¥ä¸‹ä½ç½®æ‰¾åˆ°ï¼š[javascript](https://www.tutorialspoint.com/javascript/javascript_tutorial.pdf)ã€[blockchain](https://blockchain-observatory.ec.europa.eu/document/download/1063effa-59cc-4df4-aeee-d2cf94f69178_en?filename=Blockchain_For_Beginners_A_EUBOF_Guide.pdf)ã€[microservices](https://cdn.studio.f5.com/files/k6fem79d/production/5e4126e1cefa813ab67f9c0b6d73984c27ab1502.pdf)ã€[fortinet](https://www.commoncriteriaportal.org/files/epfiles/Fortinet%20FortiGate_EAL4_ST_V1.5.pdf(320893)_TMP.pdf)  

### é€‰é¡¹ 1ï¼šå¿«é€Ÿå…¥é—¨ç¬”è®°æœ¬ï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰

æœ€ç®€å•çš„å…¥é—¨æ–¹å¼ï¼š

**åœ¨ Google Colab ä¸­è¿è¡Œï¼š**
1. ç‚¹å‡»æ­¤ README é¡¶éƒ¨çš„ **åœ¨ Colab ä¸­æ‰“å¼€** å¾½ç« 
2. åœ¨æ–‡ä»¶æµè§ˆå™¨ä¸­åˆ›å»º `docs/` æ–‡ä»¶å¤¹
3. å°†æ‚¨çš„ PDF æ–‡ä»¶ä¸Šä¼ åˆ° `docs/` æ–‡ä»¶å¤¹
4. ä»ä¸Šåˆ°ä¸‹è¿è¡Œæ‰€æœ‰å•å…ƒæ ¼
5. èŠå¤©ç•Œé¢å°†åœ¨æœ€åå‡ºç°

**æœ¬åœ°è¿è¡Œï¼ˆJupyter/VSCodeï¼‰ï¼š**
1. é¦–å…ˆå®‰è£…ä¾èµ– `pip install -r requirements.txt`
2. åœ¨æ‚¨é¦–é€‰çš„ç¯å¢ƒä¸­æ‰“å¼€ç¬”è®°æœ¬
3. å°†æ‚¨çš„ PDF æ–‡ä»¶æ·»åŠ åˆ° `docs/` æ–‡ä»¶å¤¹
4. ä»ä¸Šåˆ°ä¸‹è¿è¡Œæ‰€æœ‰å•å…ƒæ ¼
5. èŠå¤©ç•Œé¢å°†åœ¨æœ€åå‡ºç°

### é€‰é¡¹ 2ï¼šå®Œæ•´ Python é¡¹ç›®ï¼ˆæ¨èç”¨äºå¼€å‘ï¼‰

#### 1. å®‰è£…ä¾èµ–

```bash
# å…‹éš†ä»“åº“
git clone <repo-url>
cd agentic-rag-for-dummies

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv

# æ¿€æ´»å®ƒ
# åœ¨ macOS/Linux ä¸Šï¼š
source venv/bin/activate
# åœ¨ Windows ä¸Šï¼š
.\venv\Scripts\activate

# å®‰è£…åŒ…
pip install -r requirements.txt
```

#### 2. è¿è¡Œåº”ç”¨

```bash
python app.py
```

#### 3. æé—®

æ‰“å¼€æœ¬åœ° URLï¼ˆä¾‹å¦‚ `http://127.0.0.1:7860`ï¼‰å¼€å§‹èŠå¤©ã€‚

---

### é€‰é¡¹ 3ï¼šDocker éƒ¨ç½²

> âš ï¸ **ç³»ç»Ÿè¦æ±‚**ï¼šDocker éƒ¨ç½²éœ€è¦è‡³å°‘ **8GB RAM** åˆ†é…ç»™ Dockerã€‚Ollama æ¨¡å‹ï¼ˆ`qwen3:4b-instruct-2507-q4_K_M`ï¼‰éœ€è¦çº¦ 3.3GB å†…å­˜æ‰èƒ½è¿è¡Œã€‚

#### å…ˆå†³æ¡ä»¶

- åœ¨æ‚¨çš„ç³»ç»Ÿä¸Šå®‰è£… Dockerï¼ˆ[è·å– Docker](https://docs.docker.com/get-docker/)ï¼‰
- å°† Docker é…ç½®ä¸ºè‡³å°‘ 8GB RAMï¼ˆè®¾ç½® â†’ èµ„æº â†’ å†…å­˜ï¼‰

#### 1. æ„å»º Docker é•œåƒ

```bash
docker build -f project/Dockerfile -t agentic-rag .
```

#### 2. è¿è¡Œå®¹å™¨

```bash
docker run --name rag-assistant -p 7860:7860 agentic-rag
```

> âš ï¸ **æ€§èƒ½è¯´æ˜**ï¼šDocker éƒ¨ç½²å¯èƒ½æ¯”æœ¬åœ°è¿è¡Œ Python æ…¢ 20-50%ï¼Œç‰¹åˆ«æ˜¯åœ¨ Windows/Mac ä¸Šï¼Œè¿™æ˜¯ç”±äºè™šæ‹ŸåŒ–å¼€é”€å’Œ I/O æ“ä½œã€‚è¿™æ˜¯æ­£å¸¸çš„ï¼Œé¢„æœŸçš„ã€‚å¼€å‘æœŸé—´ä¸ºäº†è·å¾—æœ€å¤§æ€§èƒ½ï¼Œè¯·è€ƒè™‘ä½¿ç”¨é€‰é¡¹ 2ï¼ˆå®Œæ•´ Python é¡¹ç›®ï¼‰ã€‚

**å¯é€‰ï¼šå¯ç”¨ GPU åŠ é€Ÿ**ï¼ˆä»…é™ NVIDIA GPUï¼‰ï¼š

å¦‚æœæ‚¨æœ‰ NVIDIA GPU å’Œ [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)ï¼š

```bash
docker run --gpus all --name rag-assistant -p 7860:7860 agentic-rag
```

**å¸¸ç”¨ Docker å‘½ä»¤ï¼š**

```bash
# åœæ­¢å®¹å™¨
docker stop rag-assistant

# å¯åŠ¨ç°æœ‰å®¹å™¨
docker start rag-assistant

# å®æ—¶æŸ¥çœ‹æ—¥å¿—
docker logs -f rag-assistant

# åˆ é™¤å®¹å™¨
docker rm rag-assistant

# å¼ºåˆ¶åˆ é™¤å®¹å™¨ï¼ˆå¦‚æœæ­£åœ¨è¿è¡Œï¼‰
docker rm -f rag-assistant
```

#### 3. è®¿é—®åº”ç”¨

å®¹å™¨è¿è¡Œåï¼Œæ‚¨ä¼šçœ‹åˆ°ï¼š
```
ğŸš€ å¯åŠ¨ RAG åŠ©æ‰‹...
* è¿è¡Œåœ¨æœ¬åœ° URLï¼š  http://0.0.0.0:7860
```

æ‰“å¼€æµè§ˆå™¨å¹¶å¯¼èˆªåˆ°ï¼š
```
http://localhost:7860
```

### ç¤ºä¾‹å¯¹è¯

**å¸¦å¯¹è¯è®°å¿†ï¼š**
```
ç”¨æˆ·ï¼š"å¦‚ä½•å®‰è£… SQLï¼Ÿ"
æ™ºèƒ½ä½“ï¼š[ä»æ–‡æ¡£ä¸­æä¾›å®‰è£…æ­¥éª¤]

ç”¨æˆ·ï¼š"å¦‚ä½•æ›´æ–°å®ƒï¼Ÿ"
æ™ºèƒ½ä½“ï¼š[ç†è§£"å®ƒ" = SQLï¼Œæä¾›æ›´æ–°è¯´æ˜]
```

**å¸¦æŸ¥è¯¢æ¾„æ¸…ï¼š**
```
ç”¨æˆ·ï¼š"å‘Šè¯‰æˆ‘å…³äºé‚£ä¸ªä¸œè¥¿"
æ™ºèƒ½ä½“ï¼š"æˆ‘éœ€è¦æ›´å¤šä¿¡æ¯ã€‚æ‚¨å…·ä½“åœ¨é—®ä»€ä¹ˆä¸»é¢˜ï¼Ÿ"

ç”¨æˆ·ï¼š"PostgreSQL çš„å®‰è£…è¿‡ç¨‹"
æ™ºèƒ½ä½“ï¼š[æ£€ç´¢å¹¶å›ç­”å…·ä½“ä¿¡æ¯]
```

---

## æ•…éšœæ’é™¤

| é¢†åŸŸ | å¸¸è§é—®é¢˜ | å»ºè®®è§£å†³æ–¹æ¡ˆ |
|------|----------------|------------------|
| **æ¨¡å‹é€‰æ‹©** | - å“åº”å¿½ç•¥æŒ‡ä»¤<br>- å·¥å…·ï¼ˆæ£€ç´¢/æœç´¢ï¼‰ä½¿ç”¨ä¸æ­£ç¡®<br>- ä¸Šä¸‹æ–‡ç†è§£å·®<br>- å¹»è§‰æˆ–ä¸å®Œæ•´çš„èšåˆ | - ä½¿ç”¨æ›´å¼ºå¤§çš„ LLM<br>- æ›´å–œæ¬¢ 7B+ æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ¨ç†<br>- å¦‚æœæœ¬åœ°æ¨¡å‹æœ‰é™ï¼Œè€ƒè™‘äº‘ç«¯æ¨¡å‹ |
| **ç³»ç»Ÿæç¤ºè¡Œä¸º** | - æ¨¡å‹ä¸æ£€ç´¢æ–‡æ¡£å°±å›ç­”<br>- æŸ¥è¯¢é‡å†™ä¸¢å¤±ä¸Šä¸‹æ–‡<br>- èšåˆå¼•å…¥å¹»è§‰ | - åœ¨ç³»ç»Ÿæç¤ºä¸­æ˜ç¡®è¦æ±‚æ£€ç´¢<br>- æŸ¥è¯¢é‡å†™è´´è¿‘ç”¨æˆ·æ„å›¾<br>- å¼ºåˆ¶æ‰§è¡Œä¸¥æ ¼çš„èšåˆè§„åˆ™ |
| **æ£€ç´¢é…ç½®** | - æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£<br>- å¤ªå¤šæ— å…³ä¿¡æ¯ | - å¢åŠ æ£€ç´¢å—æ•°ï¼ˆ`k`ï¼‰æˆ–é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ä»¥æé«˜å¬å›ç‡<br>- å‡å°‘ `k` æˆ–å¢åŠ é˜ˆå€¼ä»¥æé«˜ç²¾ç¡®åº¦ |
| **å—å¤§å°/æ–‡æ¡£åˆ†å‰²** | - ç­”æ¡ˆç¼ºä¹ä¸Šä¸‹æ–‡æˆ–æ„Ÿè§‰ç¢ç‰‡åŒ–<br>- æ£€ç´¢æ…¢æˆ–åµŒå…¥æˆæœ¬é«˜ | - å¢åŠ å—å’Œçˆ¶å—å¤§å°ä»¥è·å–æ›´å¤šä¸Šä¸‹æ–‡<br>- å‡å°å—å¤§å°ä»¥æé«˜é€Ÿåº¦å¹¶é™ä½æˆæœ¬ |
| **æ¸©åº¦å’Œä¸€è‡´æ€§** | - å“åº”ä¸ä¸€è‡´æˆ–è¿‡äºæœ‰åˆ›æ„<br>- å“åº”è¿‡äºåƒµåŒ–æˆ–é‡å¤ | - å°†æ¸©åº¦è®¾ç½®ä¸º `0` ä»¥è·å¾—äº‹å®æ€§ã€ä¸€è‡´çš„è¾“å‡º<br>- ç¨å¾®å¢åŠ æ¸©åº¦ç”¨äºæ‘˜è¦æˆ–åˆ†æä»»åŠ¡ |
| **åµŒå…¥æ¨¡å‹è´¨é‡** | - è¯­ä¹‰æœç´¢å·®<br>- é¢†åŸŸç‰¹å®šæˆ–å¤šè¯­è¨€æ–‡æ¡£æ€§èƒ½å¼± | - ä½¿ç”¨æ›´é«˜è´¨é‡æˆ–é¢†åŸŸç‰¹å®šçš„åµŒå…¥<br>- æ›´æ”¹åµŒå…¥åé‡æ–°ç´¢å¼•æ‰€æœ‰æ–‡æ¡£ |

---

## è®¸å¯è¯

MIT è®¸å¯è¯ - æ¬¢è¿å°†å…¶ç”¨äºå­¦ä¹ å’Œæ„å»ºæ‚¨è‡ªå·±çš„é¡¹ç›®ï¼

---

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼Œè¯·æäº¤ issue æˆ– pull requestï¼
